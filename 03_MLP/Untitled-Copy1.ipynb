{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Packages loaded!\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "print(\"Packages loaded!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting mnist/train-images-idx3-ubyte.gz\n",
      "Extracting mnist/train-labels-idx1-ubyte.gz\n",
      "Extracting mnist/t10k-images-idx3-ubyte.gz\n",
      "Extracting mnist/t10k-labels-idx1-ubyte.gz\n",
      "Data loaded!!\n"
     ]
    }
   ],
   "source": [
    "_data_path = \"mnist/\"\n",
    "\n",
    "mnist = input_data.read_data_sets(_data_path, one_hot=True)\n",
    "print(\"Data loaded!!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "img_test = mnist.test.images\n",
    "label_test = mnist.test.labels\n",
    "\n",
    "NUM_CLASSES = 10\n",
    "IMAGE_SIZE = 28\n",
    "IMAGE_PIXEL = IMAGE_SIZE * IMAGE_SIZE\n",
    "\n",
    "BATCH_SIZE = 100\n",
    "total_batch = int(mnist.train.num_examples / BATCH_SIZE)\n",
    "\n",
    "HIDDEN1_UNITS = 128\n",
    "HIDDEN2_UNITS = 32\n",
    "\n",
    "MAX_EPOCH = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def linear(data_in, output_units, name='name'):\n",
    "    with tf.variable_scope(name):\n",
    "        input_units = data_in.get_shape().as_list()[-1]\n",
    "        weights = tf.get_variable('weights', shape=[input_units, output_units], initializer=tf.contrib.layers.xavier_initializer())\n",
    "        biases = tf.Variable(tf.zeros([output_units]), name='biases')\n",
    "        logits = tf.nn.xw_plus_b(data_in, weights, biases, name='logits')\n",
    "        activation = tf.sigmoid(logits, name=\"act\")\n",
    "    return logits, activation\n",
    "\n",
    "\n",
    "def inference_mlp1(data_in, num_hidden_node):\n",
    "    _, h1 = linear(data_in, num_hidden_node, name=\"linear1\")\n",
    "    logits, _ = linear(h1, NUM_CLASSES, name=\"linear2\")\n",
    "    return logits\n",
    "\n",
    "\n",
    "def inference_mlp2(data_in, num_hidden1_node, num_hidden2_node):\n",
    "    _, h1 = linear(data_in, num_hidden1_node, name=\"linear1\")\n",
    "    _, h2 = linear(h1, num_hidden2_node, name=\"linear2\")\n",
    "    logits, _ = linear(h2, NUM_CLASSES, name=\"linear3\")\n",
    "    return logits\n",
    "\n",
    "\n",
    "def loss(logits, labels):\n",
    "    cross_entropy = tf.nn.softmax_cross_entropy_with_logits(logits, labels, name=\"x_entropy\")\n",
    "    loss = tf.reduce_mean(cross_entropy, name=\"loss\")\n",
    "    return loss\n",
    "\n",
    "\n",
    "def training(loss, learning_rate):\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "    global_step = tf.Variable(0, name='global_step', trainable=False)\n",
    "    train = optimizer.minimize(loss, global_step)\n",
    "    return train\n",
    "\n",
    "\n",
    "def evaluation(logits, labels):\n",
    "    correct = tf.equal(tf.argmax(logits, 1), tf.argmax(labels, 1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, \"float\"))\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0: loss = 2.4273\n",
      "Train : 12.00%, Test : 9.58%\n",
      "Step 500: loss = 0.3651\n",
      "Train : 92.00%, Test : 90.81%\n",
      "Step 1000: loss = 0.2309\n",
      "Train : 97.00%, Test : 93.16%\n",
      "Step 1500: loss = 0.2897\n",
      "Train : 92.00%, Test : 93.96%\n",
      "Step 2000: loss = 0.2498\n",
      "Train : 95.00%, Test : 94.93%\n",
      "Step 2500: loss = 0.1320\n",
      "Train : 95.00%, Test : 95.58%\n",
      "Step 3000: loss = 0.1071\n",
      "Train : 98.00%, Test : 96.23%\n",
      "Step 3500: loss = 0.0239\n",
      "Train : 100.00%, Test : 96.51%\n",
      "Step 4000: loss = 0.0546\n",
      "Train : 99.00%, Test : 96.95%\n",
      "Step 4500: loss = 0.0402\n",
      "Train : 99.00%, Test : 96.94%\n",
      "Step 5000: loss = 0.0603\n",
      "Train : 99.00%, Test : 97.28%\n"
     ]
    }
   ],
   "source": [
    "NUM_GPU = 2\n",
    "\n",
    "mnist_graph = tf.Graph()\n",
    "\n",
    "with mnist_graph.as_default(), tf.device('/cpu:0'):\n",
    "    images_placeholder = []\n",
    "    labels_placeholder = []\n",
    "    for i in xrange(NUM_GPU) :\n",
    "        images_placeholder.append(tf.placeholder(tf.float32, [None, IMAGE_PIXEL]))\n",
    "        labels_placeholder.append(tf.placeholder(tf.float32, [None, NUM_CLASSES]))\n",
    "\n",
    "    #labels_placeholder = tf.placeholder(tf.float32)\n",
    "    tf.add_to_collection(\"images\", images_placeholder)\n",
    "    tf.add_to_collection(\"labels\", labels_placeholder)\n",
    "    \n",
    "    global_step = tf.get_variable('global_step', [],\n",
    "                                 initializer=tf.constant_initializer(0.001), trainable=False)\n",
    "    \n",
    "    optimizer = tf.train.AdamOptimizer(0.001)\n",
    "    \n",
    "    tower_loss = []\n",
    "    tower_grads = []\n",
    "    tower_accr = []\n",
    "    \n",
    "    for i in xrange(NUM_GPU) : \n",
    "        with ktf.device('/gpu:%d', %i) :\n",
    "            with tf.name_scope('tower_%d %i') as scope :\n",
    "                logits = inference_mlp1(image_placeholder[i[, HIDDEB1_UNITS]])\n",
    "                tf.get_variable_scope().reuse_variables()\n",
    "                cost = loss(logits, labless_placeholder),\n",
    "\n",
    "    #logits = inference_mlp1(images_placeholder, HIDDEN1_UNITS)\n",
    "    logits = inference_mlp2(images_placeholder, HIDDEN1_UNITS, HIDDEN2_UNITS)\n",
    "    tf.add_to_collection(\"logits\", logits)\n",
    "\n",
    "    cost = loss(logits, labels_placeholder)\n",
    "    train_op = training(cost, 0.001)\n",
    "\n",
    "    init = tf.initialize_all_variables()\n",
    "\n",
    "with tf.Session(graph=mnist_graph) as sess:\n",
    "    sess.run(init)\n",
    "\n",
    "    for step in xrange(MAX_EPOCH * BATCH_SIZE + 1):\n",
    "        img_batch, label_batch = mnist.train.next_batch(BATCH_SIZE)\n",
    "        feeds = {images_placeholder: img_batch, labels_placeholder: label_batch}\n",
    "        loss_value = sess.run(cost, feeds)\n",
    "        sess.run(train_op, feeds)\n",
    "\n",
    "        if step % 500 == 0:\n",
    "            print(\"Step %d: loss = %.4f\" % (step, loss_value))\n",
    "            eval_train = evaluation(logits, labels_placeholder)\n",
    "            feeds_test = {images_placeholder: mnist.test.images, labels_placeholder: mnist.test.labels}\n",
    "            eval_test = evaluation(logits, labels_placeholder)\n",
    "            print(\"Train : %.2f%%, Test : %.2f%%\" % (sess.run(eval_train, feeds)*100, sess.run(eval_test, feeds_test)*100))\n",
    "\n",
    "print(\"Done!!\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
